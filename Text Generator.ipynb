{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167516"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "content = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
    "open(\"data/wonderland.txt\", \"w\", encoding=\"utf-8\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "\n",
    "# Dataset file path\n",
    "\n",
    "FILE_PATH = \"data/wonderland.txt\"\n",
    "BASENAME = os.path.basename(FILE_PATH)\n",
    "\n",
    "# Read the data\n",
    "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
    "\n",
    "#Remove caps, leave this if uppercase is required\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans(\"\", \"\", punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_chars: \n",
      " 0123456789abcdefghijklmnopqrstuvwxyz﻿\n",
      "Number of characters :  158596\n",
      "Number of unique characters :  39\n"
     ]
    }
   ],
   "source": [
    "# Print some stats\n",
    "\n",
    "n_chars = len(text)\n",
    "vocab = ''.join(sorted(set(text)))\n",
    "print(\"unique_chars:\", vocab)\n",
    "n_unique_chars = len(vocab)\n",
    "print(\"Number of characters : \", n_chars)\n",
    "print(\"Number of unique characters : \", n_unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that converts characters to integers\n",
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "# Dictionary that converts integers to characters\n",
    "\n",
    "int2char = {i: c for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these dictionaries for later generation\n",
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text into integers\n",
    "encoded_text = np.array([char2int[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct tf.data.Dataset object\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 ﻿\n",
      "27 p\n",
      "29 r\n",
      "26 o\n",
      "21 j\n",
      "16 e\n",
      "14 c\n",
      "31 t\n"
     ]
    }
   ],
   "source": [
    "# Printing first five characters\n",
    "for char in char_dataset.take(8):\n",
    "    print(char.numpy(), int2char[char.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "this ebook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almost no restrictions whatsoever  you may copy it give it away\n",
      " or\n",
      "\n",
      "reuse it under the terms of the project gutenberg license included\n",
      "\n",
      "with this ebook or online at wwwgutenbergorg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "title alices adventures in wonderland\n",
      "\n",
      "\n",
      "\n",
      "author lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "posting date \n"
     ]
    }
   ],
   "source": [
    "# Build sequences by batching\n",
    "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
    "\n",
    "# Print sequences\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int2char[i] for i in sequence.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sample(sample):\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
    "    for i in range(1, (len(sample)-1) // 2):\n",
    "        input_ = sample[i: i+sequence_length]\n",
    "        target = sample[i+sequence_length]\n",
    "        \n",
    "        # extend dataset with samples by concatenate() method\n",
    "        \n",
    "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "        ds = ds.concatenate(other_ds)\n",
    "        \n",
    "    return ds\n",
    "\n",
    "# Prepare inputs and targets\n",
    "\n",
    "dataset = sequences.flat_map(split_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_samples(input_, target):\n",
    "    # Onehot encode both inputs and targets\n",
    "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
    "\n",
    "dataset = dataset.map(one_hot_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "this ebook is for the use of\n",
      "Target:  \n",
      "Input Shape: (100, 39)\n",
      "Target.shape: (39,)\n",
      "================================================== \n",
      "\n",
      "Input: project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "this ebook is for the use of \n",
      "Target: a\n",
      "Input Shape: (100, 39)\n",
      "Target.shape: (39,)\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing first two samples\n",
    "for element in dataset.take(2):\n",
    "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input Shape:\", element[0].shape)\n",
    "    print(\"Target.shape:\", element[1].shape)\n",
    "    print(\"=\"*50, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat, shuffle and batch the dataset\n",
    "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(n_unique_chars, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1238 steps\n",
      "1238/1238 [==============================] - 5748s 5s/step - loss: 0.0208\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
    "# save the model\n",
    "model.save(f\"results/{BASENAME}-{sequence_length}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "\n",
    "# Dataset file path\n",
    "FILE_PATH = \"data/wonderland.txt\"\n",
    "\n",
    "# FILE_PATH = \"data/python_code.py\"\n",
    "\n",
    "BASENAME = os.path.basename(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"chapter xiii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab dictionaries\n",
    "char2int = pickle.load(open(f\"{BASENAME}-char2int.pickle\", \"rb\"))\n",
    "int2char = pickle.load(open(f\"{BASENAME}-int2char.pickle\", \"rb\"))\n",
    "vocab_size = len(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model again\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(vocab_size, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimal weights \n",
    "model.load_weights(f\"results/{BASENAME}-{sequence_length}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|██████████| 400/400 [00:34<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: chapter xiii\n",
      "Generated text:\n",
      "ng of the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the pore the por\n"
     ]
    }
   ],
   "source": [
    "s = seed\n",
    "n_chars = 400\n",
    "# Generate 400 characters\n",
    "generated = \"\"\n",
    "for i in tqdm(range(n_chars), \"Generating text\"):\n",
    "    # Make the input sequence\n",
    "    X = np.zeros((1, sequence_length, vocab_size))\n",
    "    for t, char in enumerate(seed):\n",
    "        X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
    "    # predict the next character\n",
    "    predicted = model.predict(X, verbose=0)[0]\n",
    "    # converting the vector to an integer\n",
    "    next_index = np.argmax(predicted)\n",
    "    # converting the integer to a character\n",
    "    next_char = int2char[next_index]\n",
    "    # add the character to results\n",
    "    generated += next_char\n",
    "    # shift seed and the predicted character\n",
    "    seed = seed[1:] + next_char\n",
    "\n",
    "print(\"Seed:\", s)\n",
    "print(\"Generated text:\")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
